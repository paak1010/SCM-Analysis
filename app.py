import duckdb
import pandas as pd
import os
import streamlit as st
import altair as alt

# --- 1. Configuration ---
DB_FILE = 'scm.duckdb'

# CSV files needed to create the DB
# These must be in the GitHub repository
TABLES_AND_CSVS = {
    'Suppliers': 'suppliers_data.csv',
    'Products': 'products_data.csv',
    'Customers': 'customers_data.csv',
    'Orders': 'orders_data.csv',
    'Order_Details': 'order_details_data.csv'
}

# --- 2. Database Initialization (Crucial for Streamlit Cloud) ---
def initialize_database():
    """
    Checks if the DuckDB file exists. If not, creates it from CSVs.
    This runs ONCE when the Streamlit app starts on the server.
    """
    if os.path.exists(DB_FILE):
        return # DB file already exists

    print("--- Database not found. Creating from CSV files... ---")
    try:
        conn = duckdb.connect(database=DB_FILE, read_only=False)
        
        for table_name, csv_file in TABLES_AND_CSVS.items():
            if not os.path.exists(csv_file):
                # This error will show in the Streamlit logs
                print(f"Error: Missing required file: {csv_file}")
                st.error(f"Fatal Error: Missing CSV file {csv_file}. App cannot start.")
                return
            
            # Create table from CSV
            conn.execute(f"CREATE TABLE {table_name} AS SELECT * FROM read_csv_auto('{csv_file}', header=True)")
            print(f"Successfully created table: {table_name}")

        print("--- Database initialization complete. ---")
        conn.close()

    except Exception as e:
        print(f"Error during DB initialization: {e}")
        st.error(f"Database creation failed: {e}")

# Run the initialization ONCE at the start
initialize_database()


# --- 3. Database Connection & Data Fetching (with Caching) ---

# Use st.cache_resource to cache the database connection
@st.cache_resource
def get_db_connection():
    """Gets a cached connection to the DuckDB file."""
    try:
        conn = duckdb.connect(database=DB_FILE, read_only=True)
        return conn
    except Exception as e:
        st.error(f"Failed to connect to DuckDB: {e}")
        return None

# Use st.cache_data to cache the results of data queries
@st.cache_data
def get_all_products(_conn):
    """Fetches all product names and IDs for the selector."""
    try:
        products_df = _conn.execute("SELECT ProductID, ProductName FROM Products ORDER BY ProductName").df()
        return products_df
    except Exception as e:
        st.error(f"Error fetching product list: {e}")
        return pd.DataFrame(columns=["ProductID", "ProductName"])

@st.cache_data
def get_sales_history(_conn, product_id):
    """Fetches and aggregates monthly sales for a specific product."""
    query = f"""
    SELECT 
        strftime(o.OrderDate, '%Y-%m') AS SalesMonth,
        SUM(od.Quantity) AS TotalQuantity
    FROM Order_Details od
    JOIN Orders o ON od.OrderID = o.OrderID
    WHERE od.ProductID = {product_id}
    GROUP BY SalesMonth
    ORDER BY SalesMonth;
    """
    sales_df = _conn.execute(query).df()
    # Ensure SalesMonth is a datetime object for charting
    if not sales_df.empty:
        sales_df['SalesMonth'] = pd.to_datetime(sales_df['SalesMonth'])
    return sales_df

@st.cache_data
def get_product_analysis_details(_conn, product_id):
    """Fetches all details needed for ROP calculation."""
    query = f"""
    SELECT 
        p.ProductName,
        p.StockQuantity,
        p.SafetyStockLevel,
        s.LeadTimeDays
    FROM Products p
    JOIN Suppliers s ON p.SupplierID = s.SupplierID
    WHERE p.ProductID = {product_id};
    """
    details = _conn.execute(query).fetchone()
    if details:
        return {
            "name": details[0],
            "stock": details[1],
            "safety_stock": details[2],
            "lead_time": details[3]
        }
    return None

# --- 4. Analysis & Forecasting Logic ---

def calculate_rop(sales_df, details):
    """Calculates ROP and provides analysis."""
    if sales_df.empty:
        return 0, 0, "íŒë§¤ ì´ë ¥ ì—†ìŒ"

    avg_monthly_sales = sales_df['TotalQuantity'].mean()
    avg_daily_demand = avg_monthly_sales / 30.0
    
    lead_time = details["lead_time"]
    safety_stock = details["safety_stock"]
    
    demand_during_lead_time = avg_daily_demand * lead_time
    reorder_point = demand_during_lead_time + safety_stock
    
    return avg_daily_demand, reorder_point, "ë¶„ì„ ì™„ë£Œ"


# --- 5. Streamlit App UI ---

# Set page title and layout
st.set_page_config(page_title="SCM ì¬ê³  ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ", layout="wide")

# Get DB connection
conn = get_db_connection()

if conn is None:
    st.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•±ì„ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.")
else:
    # --- Sidebar ---
    st.sidebar.title("SCM Dashboard")
    st.sidebar.image("https://placehold.co/400x200/06B6D4/FFFFFF?text=SCM+Model", use_column_width=True)
    
    product_list_df = get_all_products(conn)
    
    # Create a mapping from "Name (ID)" to just ID
    product_options = {f"{row.ProductName} (ID: {row.ProductID})": row.ProductID for index, row in product_list_df.iterrows()}
    
    selected_option = st.sidebar.selectbox(
        "ë¶„ì„í•  ì œí’ˆì„ ì„ íƒí•˜ì„¸ìš”:",
        options=list(product_options.keys())
    )
    
    # Get the ID from the selected option
    selected_product_id = product_options[selected_option]

    # --- Main Page ---
    st.title(f"ğŸ“ˆ SCM ìˆ˜ìš” ì˜ˆì¸¡ ë° ì¬ê³  ë¶„ì„")
    st.markdown(f"í˜„ì¬ ì„ íƒëœ ì œí’ˆ: **{selected_option}**")
    
    # --- Fetch data for the selected product ---
    details = get_product_analysis_details(conn, selected_product_id)
    sales_history_df = get_sales_history(conn, selected_product_id)
    
    if details is None:
        st.error("ì œí’ˆ ìƒì„¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        # --- Run analysis ---
        avg_daily_demand, reorder_point, status = calculate_rop(sales_history_df, details)
        
        # --- Display Key Metrics ---
        st.header("ğŸ“Š í•µì‹¬ ì¬ê³  ì§€í‘œ (KPIs)")
        col1, col2, col3 = st.columns(3)
        col1.metric("í˜„ì¬ ì¬ê³  (Stock)", f"{details['stock']} ê°œ")
        col2.metric("ì•ˆì „ ì¬ê³  (Safety Stock)", f"{details['safety_stock']} ê°œ")
        col3.metric("ê³µê¸‰ì ë¦¬ë“œíƒ€ì„ (Lead Time)", f"{details['lead_time']} ì¼")

        st.divider()

        # --- Display Analysis Result ---
        st.header("ğŸ’¡ ë¶„ì„ ê²°ê³¼: ì¬ì£¼ë¬¸ì  (ROP)")
        
        col_rop, col_demand = st.columns(2)
        col_rop.metric("ê³„ì‚°ëœ ì¬ì£¼ë¬¸ì  (Reorder Point)", f"{reorder_point:.1f} ê°œ")
        col_demand.metric("ì˜ˆì¸¡ ì¼í‰ê·  ìˆ˜ìš” (Daily Demand)", f"{avg_daily_demand:.1f} ê°œ/ì¼")

        # --- Final Verdict ---
        current_stock = details['stock']
        if current_stock < reorder_point:
            st.error(f"**[ì¡°ì¹˜ í•„ìš”]** í˜„ì¬ ì¬ê³ ({current_stock})ê°€ ì¬ì£¼ë¬¸ì ({reorder_point:.1f})ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤. **ì¦‰ì‹œ ë°œì£¼ê°€ í•„ìš”í•©ë‹ˆë‹¤!**")
        else:
            st.success(f"**[ì–‘í˜¸]** í˜„ì¬ ì¬ê³ ({current_stock})ê°€ ì¬ì£¼ë¬¸ì ({reorder_point:.1f})ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤. ì¬ê³ ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤.")

        # --- Display Sales History Chart ---
        st.header("ğŸ“‰ ê³¼ê±° íŒë§¤ ì´ë ¥ (ì›”ë³„)")
        if not sales_history_df.empty:
            # Create an Altair chart
            chart = alt.Chart(sales_history_df).mark_bar(color="#06B6D4").encode(
                x=alt.X('SalesMonth', title='ì›”'),
                y=alt.Y('TotalQuantity', title='ì´ íŒë§¤ëŸ‰'),
                tooltip=['SalesMonth', 'TotalQuantity']
            ).properties(
                title=f"{details['name']} ì›”ë³„ íŒë§¤ëŸ‰"
            ).interactive()
            
            st.altair_chart(chart, use_container_width=True)
        else:
            st.warning("ì´ ì œí’ˆì€ ì•„ì§ íŒë§¤ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")